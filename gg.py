# -*- coding: utf-8 -*-
"""GG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wgCi9etz4yZvpjJvvt0d1g1vkMFlzHXb
"""

!unzip archive.zip

import pandas as pd

df = pd.read_csv('creditcard.csv')

print("Shape of the DataFrame:", df.shape)
print(df.head())

import seaborn as sns
import matplotlib.pyplot as plt

# the number of each category
print(df['Class'].value_counts())

# diagram
sns.countplot(x='Class', data=df)
plt.title('Class Distribution (0 = Non-Fraud, 1 = Fraud)')
plt.xlabel('Class')
plt.ylabel('Count')
plt.show()

# Calculating percentages
class_counts = df['Class'].value_counts(normalize=True)
print("\nClass Percentages:")
print(class_counts)

#(minority class)
df_fraud = df[df['Class'] == 1]

#(majority class)
df_non_fraud = df[df['Class'] == 0]

#the number of samples in each category
print("Number of fraud samples:", len(df_fraud))
print("Number of non-fraud samples:", len(df_non_fraud))

from sklearn.preprocessing import StandardScaler

# بشيل ال الأعمدة الغير ضرورية'Time' و'Class'
df_fraud_features = df_fraud.drop(['Time', 'Class'], axis=1)

#Normalization
scaler = StandardScaler()
X_fraud_scaled = scaler.fit_transform(df_fraud_features)

print("Scaled fraud data shape:", X_fraud_scaled.shape)

import torch

# تحويل الداتا Tensor
X_tensor = torch.tensor(X_fraud_scaled, dtype=torch.float32)

import torch
import torch.nn as nn

# Generator
class Generator(nn.Module):
    def __init__(self, latent_dim=100, input_dim=29):
        super(Generator, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(latent_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Linear(512, input_dim),
            nn.Tanh()  )
            #values between -1 , 1

    def forward(self, z):
        return self.model(z)

# Discriminator (ببين اذا الداتا حقيقية  ولا مزيفة)
class Discriminator(nn.Module):
    def __init__(self, input_dim=29):
        super(Discriminator, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.model(x)

#Data dimensions
latent_dim = 100
input_dim = X_tensor.shape[1]  # عدد ال features

#creat models
generator = Generator(latent_dim, input_dim)
discriminator = Discriminator(input_dim)

# Loss Function
criterion = nn.BCELoss()

# Optimizers
g_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0002)
d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.0002)

# DataLoader )(عشان نقسم الداتا لأكتر من دفعة)
from torch.utils.data import DataLoader, TensorDataset
loader = DataLoader(TensorDataset(X_tensor), batch_size=64, shuffle=True)

#epochs
num_epochs = 100

for epoch in range(num_epochs):
    for real_batch in loader:
        real_data = real_batch[0]
        batch_size = real_data.size(0)

        # Labels for real and fake data
        real_labels = torch.ones(batch_size, 1)
        fake_labels = torch.zeros(batch_size, 1)

        # Train Discriminator
        d_optimizer.zero_grad()

        # Loss for real data
        real_output = discriminator(real_data)
        d_loss_real = criterion(real_output, real_labels)

        # Generate random noise and fake data
        z = torch.randn(batch_size, latent_dim)
        fake_data = generator(z)

        # Loss for fake data
        fake_output = discriminator(fake_data.detach())
        d_loss_fake = criterion(fake_output, fake_labels)

         # Total loss for the discriminator
        d_loss = d_loss_real + d_loss_fake
        d_loss.backward()
        d_optimizer.step()

        # Train Generator
        g_optimizer.zero_grad()

        # Generator tries to fool the discriminator into thinking fake data is real!
        fake_output = discriminator(fake_data)
        g_loss = criterion(fake_output, real_labels)
        g_loss.backward()
        g_optimizer.step()

   # Print results every 20 epochs
    if (epoch + 1) % 20 == 0:
        print(f"Epoch [{epoch+1}/{num_epochs}], D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}")

# Number of samples to generate
num_samples = 500

# Generate random noise
z = torch.randn(num_samples, latent_dim)

# Generate synthetic data
generated_data = generator(z).detach().numpy()

# Revert values back to original scale (Inverse transform)
generated_data_original = scaler.inverse_transform(generated_data)

# Show the shape of generated data
print("Shape of generated data:", generated_data_original.shape)

# show the first 5 rows
import pandas as pd

# Make sure df_fraud does not include Time and Class columns
df_fraud_clean = df[df['Class'] == 1].drop(['Time', 'Class'], axis=1)

# Create a DataFrame with the generated data
generated_df = pd.DataFrame(generated_data_original, columns=df_fraud_clean.columns)

# Add a new column Class = 1 (for synthetic fraud data)
generated_df['Class'] = 1

# Show the generated samples
print("\nSample of Generated Data:")
print(generated_df.head())

# Split the original data into non-fraud and fraud
df_non_fraud = df[df['Class'] == 0] #all non-fraud transactions (Class=0)
df_fraud_original = df[df['Class'] == 1] #synthetic fraud data (Class=1)

# Combine the data to create a balanced dataset
balanced_df = pd.concat([df_non_fraud, df_fraud_original, generated_df], axis=0)

# Shuffle the rows randomly
balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)

# Show class distribution after balancing
print(balanced_df['Class'].value_counts())

import seaborn as sns
import matplotlib.pyplot as plt

sns.countplot(x='Class', data=balanced_df)
plt.title('Balanced Class Distribution')
plt.show()

num_samples = 10000  #Increase the number of generated samples
z = torch.randn(num_samples, latent_dim)
generated_data = generator(z).detach().numpy()
generated_data_original = scaler.inverse_transform(generated_data)

generated_df = pd.DataFrame(generated_data_original, columns=df_fraud_clean.columns)
generated_df['Class'] = 1

balanced_df = pd.concat([df_non_fraud, df_fraud_original, generated_df], axis=0)
balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)

print(balanced_df['Class'].value_counts())

# Generator و Discriminator for WGAN
import torch
import torch.nn as nn

class WGAN_Generator(nn.Module):
    def __init__(self, latent_dim=100, input_dim=29):
        super(WGAN_Generator, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(latent_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Linear(512, input_dim),
            nn.Tanh()
        )

    def forward(self, z):
        return self.model(z)


class WGAN_Critic(nn.Module):  #  بنستخدم ال (Critic) بدل (Discriminator)
    def __init__(self, input_dim=29):
        super(WGAN_Critic, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 1)
        )

    def forward(self, x):
        return self.model(x)

latent_dim = 100
input_dim = X_tensor.shape[1]

generator_wgan = WGAN_Generator(latent_dim, input_dim)
critic_wgan = WGAN_Critic(input_dim)

g_optimizer_wgan = torch.optim.RMSprop(generator_wgan.parameters(), lr=0.00005)
c_optimizer_wgan = torch.optim.RMSprop(critic_wgan.parameters(), lr=0.00005)

# DataLoader
from torch.utils.data import DataLoader, TensorDataset
loader = DataLoader(TensorDataset(X_tensor), batch_size=64, shuffle=True)

#Training Loop
num_epochs_wgan = 200
n_critic = 5 # Number of times Critic is trained per epoch

for epoch in range(num_epochs_wgan):
    for i, real_batch in enumerate(loader):
        real_data = real_batch[0]
        batch_size = real_data.size(0)

        #Train Critic
        for _ in range(n_critic):
            c_optimizer_wgan.zero_grad()

            # Real loss
            real_loss = -torch.mean(critic_wgan(real_data))

            # Fake loss
            z = torch.randn(batch_size, latent_dim)
            fake_data = generator_wgan(z)
            fake_loss = torch.mean(critic_wgan(fake_data))

            # Total loss
            c_loss = real_loss + fake_loss
            c_loss.backward()
            c_optimizer_wgan.step()

            # Clip critic weights
            for p in critic_wgan.parameters():
                p.data.clamp_(-0.01, 0.01)

        #Train Generator
        g_optimizer_wgan.zero_grad()
        z = torch.randn(batch_size, latent_dim)
        fake_data = generator_wgan(z)
        g_loss = -torch.mean(critic_wgan(fake_data))
        g_loss.backward()
        g_optimizer_wgan.step()

    if (epoch + 1) % 50 == 0:
        print(f"Epoch [{epoch+1}/{num_epochs_wgan}], C Loss: {c_loss.item():.4f}, G Loss: {g_loss.item():.4f}")

# Number of samples to generate
num_samples_wgan = 500

# Generate random noise
z_wgan = torch.randn(num_samples_wgan, latent_dim)

# Generate synthetic data
generated_data_wgan = generator_wgan(z_wgan).detach().numpy()

# Revert values back to original scale
generated_data_original_wgan = scaler.inverse_transform(generated_data_wgan)

# Show the shape of generated data
print("Shape of generated data from WGAN:", generated_data_original_wgan.shape)

# creat DataFrame
generated_df_wgan = pd.DataFrame(generated_data_original_wgan, columns=df_fraud_clean.columns)
generated_df_wgan['Class'] = 1

# VARIANT 2: LSGAN (Least Squares GAN)
import torch
import torch.nn as nn

# LSGAN Discriminator
class LSGAN_Discriminator(nn.Module):
    def __init__(self, input_dim=29):
        super(LSGAN_Discriminator, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 1)
        )

    def forward(self, x):
        return self.model(x)

generator_lsgan = Generator(latent_dim, input_dim)
discriminator_lsgan = LSGAN_Discriminator(input_dim)

criterion_lsgan = nn.MSELoss()

# (Optimizers)
g_optimizer_lsgan = torch.optim.Adam(generator_lsgan.parameters(), lr=0.0002)
d_optimizer_lsgan = torch.optim.Adam(discriminator_lsgan.parameters(), lr=0.0002)

#   (Training Loop)
print("Starting LSGAN Training...")
num_epochs_lsgan = 100

for epoch in range(num_epochs_lsgan):
    for real_batch in loader:
        real_data = real_batch[0]
        batch_size = real_data.size(0)

        # LSGAN Labels: 1 للحقيقي و 0 للمزيف (قيم رقمية وليست احتمالات)
        real_labels = torch.ones(batch_size, 1)
        fake_labels = torch.zeros(batch_size, 1)

        # --- Discriminator ---
        d_optimizer_lsgan.zero_grad()

        real_output = discriminator_lsgan(real_data)
        d_loss_real = criterion_lsgan(real_output, real_labels)

        z = torch.randn(batch_size, latent_dim)
        fake_data = generator_lsgan(z)
        fake_output = discriminator_lsgan(fake_data.detach()) # detach لفصل الغراديانت عن المولد
        d_loss_fake = criterion_lsgan(fake_output, fake_labels)

        d_loss = 0.5 * (d_loss_real + d_loss_fake)
        d_loss.backward()
        d_optimizer_lsgan.step()

        # --- Generator ---
        g_optimizer_lsgan.zero_grad()

        fake_output = discriminator_lsgan(fake_data)
        g_loss = criterion_lsgan(fake_output, real_labels)

        g_loss.backward()
        g_optimizer_lsgan.step()

    if (epoch + 1) % 20 == 0:
        print(f"Epoch [{epoch+1}/{num_epochs_lsgan}], D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}")

# --- LSGAN ---
z_lsgan = torch.randn(num_samples, latent_dim)
generated_data_lsgan = generator_lsgan(z_lsgan).detach().numpy()
generated_data_original_lsgan = scaler.inverse_transform(generated_data_lsgan)

generated_df_lsgan = pd.DataFrame(generated_data_original_lsgan, columns=df_fraud_clean.columns)
generated_df_lsgan['Class'] = 1

print("LSGAN Data Generated. Shape:", generated_df_lsgan.shape)

# --- (Updated Evaluation) ---
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score
import matplotlib.pyplot as plt
import seaborn as sns

def prepare_data(df):
    X = df.drop('Class', axis=1)
    y = df['Class']
    return train_test_split(X, y, test_size=0.2, random_state=42)

def evaluate_model(X_train, y_train, X_test, y_test):
    model = RandomForestClassifier(n_estimators=30, max_depth=10, n_jobs=-1, random_state=42)

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    return [
        precision_score(y_test, y_pred),
        recall_score(y_test, y_pred),
        f1_score(y_test, y_pred),
        roc_auc_score(y_test, y_pred)
    ]

# 1. تدريب على البيانات الأصلية
print("Training on Original Data...")
X_train_orig, X_test_orig, y_train_orig, y_test_orig = prepare_data(df)
metrics_orig = evaluate_model(X_train_orig, y_train_orig, X_test_orig, y_test_orig)

# 2. تدريب على Vanilla GAN
print("Training on Vanilla GAN Balanced Data...")
balanced_vanilla = pd.concat([df_non_fraud, df_fraud_original, generated_df], axis=0).sample(frac=1, random_state=42)
X_train_van, X_test_van, y_train_van, y_test_van = prepare_data(balanced_vanilla)
metrics_vanilla = evaluate_model(X_train_van, y_train_van, X_test_van, y_test_van)

# 3. تدريب على WGAN
print("Training on WGAN Balanced Data...")
balanced_wgan = pd.concat([df_non_fraud, df_fraud_original, generated_df_wgan], axis=0).sample(frac=1, random_state=42)
X_train_wgan, X_test_wgan, y_train_wgan, y_test_wgan = prepare_data(balanced_wgan)
metrics_wgan = evaluate_model(X_train_wgan, y_train_wgan, X_test_wgan, y_test_wgan)

# 4. تدريب على LSGAN
print("Training on LSGAN Balanced Data...")
balanced_lsgan = pd.concat([df_non_fraud, df_fraud_original, generated_df_lsgan], axis=0).sample(frac=1, random_state=42)
X_train_ls, X_test_ls, y_train_ls, y_test_ls = prepare_data(balanced_lsgan)
metrics_lsgan = evaluate_model(X_train_ls, y_train_ls, X_test_ls, y_test_ls)

results = {
    'Metric': ['Precision', 'Recall', 'F1-Score', 'ROC-AUC'],
    'Original': metrics_orig,
    'Vanilla GAN': metrics_vanilla,
    'WGAN': metrics_wgan,
    'LSGAN': metrics_lsgan
}

results_df = pd.DataFrame(results)
print("\nFinal Comparison Results:")
print(results_df)

results_melted = results_df.melt(id_vars='Metric', var_name='Dataset', value_name='Score')
plt.figure(figsize=(12, 6))
sns.barplot(x='Metric', y='Score', hue='Dataset', data=results_melted, errorbar=None)
plt.title('Performance Comparison: Original vs Vanilla vs WGAN vs LSGAN')
plt.ylim(0, 1.1)
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()

from sklearn.decomposition import PCA
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# --- 1. PCA Visualization  ---
def plot_pca(real, fake, title):
    common_cols = real.columns.intersection(fake.columns)
    real = real[common_cols]
    fake = fake[common_cols]

    real = real.replace([np.inf, -np.inf], np.nan).dropna()
    fake = fake.replace([np.inf, -np.inf], np.nan).dropna()

    min_len = min(len(real), len(fake))
    if min_len < 2:
        print(f"Skipping PCA for {title}: Not enough data after cleaning.")
        return

    real = real.sample(n=min_len, random_state=42)
    fake = fake.sample(n=min_len, random_state=42)

    pca = PCA(n_components=2)
    combined = pd.concat([real, fake], axis=0)
    pca_res = pca.fit_transform(combined)

    real_pca = pca_res[:len(real)]
    fake_pca = pca_res[len(real):]

    plt.figure(figsize=(8, 5))
    plt.scatter(real_pca[:, 0], real_pca[:, 1], alpha=0.5, label='Real Fraud', c='blue')
    plt.scatter(fake_pca[:, 0], fake_pca[:, 1], alpha=0.5, label='Generated Fraud', c='orange')
    plt.title(title)
    plt.legend()
    plt.show()

cols_to_drop = ['Class', 'Time']
existing_cols_to_drop = [c for c in cols_to_drop if c in df_fraud_original.columns]
real_sample_full = df_fraud_original.drop(existing_cols_to_drop, axis=1)

# Vanilla GAN
if 'generated_df' in globals():

    fake_sample = generated_df.drop('Class', axis=1)
    plot_pca(real_sample_full, fake_sample, "PCA: Real vs Vanilla GAN")

#  WGAN
if 'generated_df_wgan' in globals():
    fake_sample = generated_df_wgan.drop('Class', axis=1)
    plot_pca(real_sample_full, fake_sample, "PCA: Real vs WGAN")

#  LSGAN
if 'generated_df_lsgan' in globals():
    fake_sample = generated_df_lsgan.drop('Class', axis=1)
    plot_pca(real_sample_full, fake_sample, "PCA: Real vs LSGAN")


# --- 2. Confusion Matrices (مصفوفات الارتباك) ---
def plot_cm(model, X_test, y_test, title):
    y_pred = model.predict(X_test)
    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Normal', 'Fraud'])
    disp.plot(cmap=plt.cm.Blues)
    plt.title(title)
    plt.show()

if 'model_orig' in globals():
    plot_cm(model_orig, X_test_orig, y_test_orig, "Confusion Matrix - Original")

if 'model_lsgan' in globals():
    plot_cm(model_lsgan, X_test_ls, y_test_ls, "Confusion Matrix - LSGAN (Best Performer)")